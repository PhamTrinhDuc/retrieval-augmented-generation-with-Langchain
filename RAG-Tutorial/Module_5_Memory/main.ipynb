{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. End-to-End Example with LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Ngày độc lập của nước Việt Nam là ngày nào ?\n",
      "History:  \n",
      "Answer:  Chào bạn! Tôi rất vui khi được trả lời câu hỏi của bạn.\n",
      "\n",
      "Ngày độc lập của nước Việt Nam là ngày 2 tháng 9 năm 1945. Đây là ngày Chủ tịch Hồ Chí Minh đọc Tuyên ngôn Độc lập tại Quảng trường Ba Đình, Hà Nội, tuyên bố Việt Nam độc lập khỏi chế độ thực dân Pháp và thành lập Nhà nước Dân chủ Cộng hòa Việt Nam.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "template = \"\"\"Bạn là 1 chat bot hữu ích và là 1 chuyên gia về lịch sử thế giới.\n",
    "Hãy trả lời câu hỏi từ người dùng: {question} \n",
    "Lưu ý: bạn chỉ được sử dụng tiếng việt để trả lời.\"\"\"\n",
    "prompt = PromptTemplate(template=template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "response = conversation({\"question\": \"Ngày độc lập của nước Việt Nam là ngày nào ?\"})\n",
    "print(\"Question: \", response['question'])\n",
    "print(\"History: \", response['chat_history'])\n",
    "print(\"Answer: \", response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Memory Types in Langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Conversation Buffer Memory**\n",
    "Loại bộ nhớ này cho phép bạn lưu trữ và trích xuất tin nhắn từ các cuộc hội thoại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Xin chào\\nAI: Có chuyện gì vậy ?\\nHuman: bạn có phải là 1 chatbot không?\\nAI: Đúng vậy, tôi là 1 chatbot'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Xin chào\"}, \n",
    "                    {\"output\": \"Có chuyện gì vậy ?\"})\n",
    "memory.save_context({\"input\": \"bạn có phải là 1 chatbot không?\"}, \n",
    "                    {\"output\": \"Đúng vậy, tôi là 1 chatbot\"})\n",
    "history = memory.load_memory_variables({})\n",
    "# Extract history as a string\n",
    "print(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Conversation Buffer Window Memory**\n",
    "Loại bộ nhớ này giữ một danh sách các tương tác gần đây và sử dụng K tương tác cuối cùng, ngăn không cho bộ đệm trở nên quá lớn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: bạn có phải là 1 chatbot không?\\nAI: Đúng vậy, tôi là 1 chatbot'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "memory.save_context({\"input\": \"Xin chào\"}, \n",
    "                    {\"output\": \"Có chuyện gì vậy ?\"})\n",
    "memory.save_context({\"input\": \"bạn có phải là 1 chatbot không?\"}, \n",
    "                    {\"output\": \"Đúng vậy, tôi là 1 chatbot\"})\n",
    "history = memory.load_memory_variables({})\n",
    "# Extract history as a string\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Conversation Entity Memory**\n",
    "Loại bộ nhớ này ghi nhớ sự thật về các thực thể cụ thể trong cuộc trò chuyện và trích xuất thông tin bằng LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Deven và Sam đang cùng nhau làm 1 project trên Hackathon\n",
      "AI: Nghe có vẻ như một dự án tuyệt vời! Họ đang thực hiện loại dự án nào?\n",
      "{'The output is: Deven': ''}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "_input = {\"input\": \"Deven và Sam đang cùng nhau làm 1 project trên Hackathon\"}\n",
    "memory.load_memory_variables(_input)\n",
    "memory.save_context(_input, \n",
    "                    {\"output\": \"Nghe có vẻ như một dự án tuyệt vời! Họ đang thực hiện loại dự án nào?\"})\n",
    "\n",
    "# _input = {\"input\": \"Họ đang thực hiện 1 dự án về phân loại hình ảnh\"}\n",
    "# memory.load_memory_variables(_input)\n",
    "# memory.save_context(_input, \n",
    "#                     {\"output\": \"Thật tuyệt. Đó là 1 dự án hữu ích\"})\n",
    "\n",
    "history = memory.load_memory_variables({\"input\": 'Deven là ai ?'})\n",
    "# Extract history as a string\n",
    "print(history['history'])\n",
    "print(history['entities'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Conversation Knowledge Graph Memory**\n",
    "Loại bộ nhớ này sử dụng biểu đồ tri thức để tạo lại bộ nhớ. Bạn có thể trích xuất các thực thể hiện tại và bộ ba kiến ​​thức từ tin nhắn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "\n",
    "memory.save_context({\"input\": \"say hi to sam\"}, {\"output\": \"who is sam\"})\n",
    "memory.save_context({\"input\": \"sam is a friend\"}, {\"output\": \"okay\"})\n",
    "memory.load_memory_variables({\"input\": \"who is sam\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Conversation Summary Memory**\n",
    "Loại bộ nhớ này tạo ra bản tóm tắt cuộc trò chuyện theo thời gian, hữu ích cho việc cô đọng thông tin từ các cuộc trò chuyện dài."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Here is the new summary:\\n\\nThe human greets the AI in Vietnamese, saying \"Xin chào\". The AI responds with a greeting, \"Xin chào\", and asks how it can assist the human. The human asks if the AI is a chatbot, and the AI confirms it is a virtual assistant, offering to help answer any questions the human may have.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory.save_context({\"input\": \"Xin chào\"}, {\"output\": \"Xin chào. Tôi có thể giúp gì cho bạn\"})\n",
    "memory.save_context({\"input\": \"Bạn là 1 chatbot phải không ?\"}, {\"output\": \"Đúng vậy tôi là 1 trợ lý ảo. Hãy đưa ra thắc mắc, tôi sẽ giúp bạn trả lời.\"})\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6 VectorStoreRetrieverMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741aee1e46a34c9ea5d43182ff233444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043add38401b4c3faee7b7222e2cdc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0a56882a8e47699330c9934cf4f34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece30987c42c42e29113752b0c6f10b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cfcc04a8364c5e97b10b7acd38d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f99feff1eda490ca8558f54d97b49d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: My favorite sport is soccer\n",
      "output: ...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings import FastEmbedEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_size = 384 # dimension of the embedding model\n",
    "index =faiss.IndexFlatL2(embedding_size)\n",
    "embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\").embed_query\n",
    "vector_store = FAISS(embedding_model, \n",
    "                     index,\n",
    "                     InMemoryDocstore({}), {})\n",
    "\n",
    "# Create VectorStoreRetrieverMemory\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# Save context and relevant information to the memory\n",
    "memory.save_context({\"input\": \"My favorite food is pizza\"}, {\"output\": \"that's good to know\"})\n",
    "memory.save_context({\"input\": \"My favorite sport is soccer\"}, {\"output\": \"...\"})\n",
    "memory.save_context({\"input\": \"I don't like the Celtics\"}, {\"output\": \"ok\"})\n",
    "\n",
    "\n",
    "print(memory.load_memory_variables({\"prompt\": \"what sport should i watch?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Combine memory with LLMChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Bạn là một chatbot hữu ích đang trò chuyện với con người. Lưu ý: bạn chỉ được sử dụng tiếng việt để trò chuyện\n",
      "Human: Xin chào bạn của tôi.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Xin chào! Tôi rất vui được trò chuyện với bạn. Bạn cần giúp đỡ gì hoặc chỉ muốn tâm sự với tôi?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"Bạn là một chatbot hữu ích đang trò chuyện với con người. Lưu ý: bạn chỉ được sử dụng tiếng việt để trò chuyện\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # Where the human input will be injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = chain.predict(human_input=\"Xin chào bạn của tôi.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Xin chào bạn của tôi.'),\n",
       " AIMessage(content='Xin chào! Tôi rất vui được trò chuyện với bạn. Bạn cần giúp đỡ gì hoặc chỉ muốn tâm sự với tôi?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Bạn là một chatbot hữu ích đang trò chuyện với con người. Lưu ý: bạn chỉ được sử dụng tiếng việt để trò chuyện\n",
      "Human: Xin chào bạn của tôi.\n",
      "AI: Xin chào! Tôi rất vui được trò chuyện với bạn. Bạn cần giúp đỡ gì hoặc chỉ muốn tâm sự với tôi?\n",
      "Human: hãy nói tổng quan về Việt Nam\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Việt Nam! Đất nước tôi yêu quý!\n",
      "\n",
      "Việt Nam là một nước Đông Nam Á, giáp với Trung Quốc phía bắc, Lào và Campuchia phía tây, biển Đông phía đông và nam. Đất nước chúng ta có hình chữ S, dài khoảng 1.700 km, rộng khoảng 500 km.\n",
      "\n",
      "Việt Nam có diện tích khoảng 331.212 km², dân số hơn 98 triệu người, với hơn 54 dân tộc anh em, trong đó người Kinh chiếm đa số. Thủ đô của Việt Nam là Hà Nội, thành phố lớn nhất là Thành phố Hồ Chí Minh.\n",
      "\n",
      "Việt Nam có một vị trí địa lý chiến lược, với bờ biển dài hơn 3.000 km, nhiều cảng biển quan trọng, và là một phần của con đường tơ lụa trên biển. Đất nước chúng ta có tài nguyên thiên nhiên khá phong phú, với dầu mỏ, khí đốt, than đá, sắt, đồng, chì, kẽm, và nhiều khoáng sản khác.\n",
      "\n",
      "Việt Nam có một lịch sử lâu dài và phức tạp, với hơn 4.000 năm lịch sử, và đã trải qua nhiều giai đoạn từ triều đại phong kiến đến chiến tranh giành độc lập. Ngày nay, Việt Nam là một nước xã hội chủ nghĩa, đang trên con đường phát triển kinh tế và hội nhập quốc tế.\n",
      "\n",
      "Việt Nam cũng là một nước có nhiều di sản văn hóa và tự nhiên được UNESCO công nhận, như Vịnh Hạ Long, Phong Nha - Kẻ Bàng, Hội An, Mỹ Sơn, và nhiều di tích lịch sử khác.\n",
      "\n",
      "Ngoài ra, Việt Nam cũng là một nước có nhiều món ăn ngon và hấp dẫn, như phở, bún, bánh mì, và nhiều món ăn khác...\n",
      "\n",
      "Bạn có muốn biết thêm về Việt Nam không?\n"
     ]
    }
   ],
   "source": [
    "response = chain.predict(human_input = \"hãy nói tổng quan về Việt Nam\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Bạn là một chatbot hữu ích đang trò chuyện với con người. Lưu ý: bạn chỉ được sử dụng tiếng việt để trò chuyện\n",
      "Human: Xin chào bạn của tôi.\n",
      "AI: Xin chào! Tôi rất vui được trò chuyện với bạn. Bạn đang cần giúp đỡ gì hoặc chỉ muốn tám chuyện thư giãn?\n",
      "Human: hãy nói tổng quan về Việt Nam\n",
      "AI: Việt Nam là một nước đẹp và giàu truyền thống! Nước ta nằm ở Đông Nam Á, có biên giới với Trung Quốc, Lào, Campuchia, và biển Đông.\n",
      "\n",
      "Việt Nam có diện tích khoảng 331.212 km², với hơn 98 triệu người, bao gồm 54 dân tộc khác nhau. Tiếng Việt là ngôn ngữ chính thức, và đa số người dân theo đạo Phật.\n",
      "\n",
      "Nước ta có một lịch sử lâu đời và phong phú, với nhiều di tích lịch sử và văn hóa quý giá như Hoàng thành Thăng Long, Kinh thành Huế, và khu di tích Mỹ Sơn. Ngoài ra, Việt Nam còn có nhiều thắng cảnh thiên nhiên đẹp như Vịnh Hạ Long, Phong Nha - Kẻ Bàng, và Núi Bà Đen.\n",
      "\n",
      "Việt Nam cũng là một nước có nền kinh tế đang phát triển, với tốc độ tăng trưởng kinh tế khá cao trong những năm gần đây. Nước ta có nhiều ngành kinh tế mũi nhọn như công nghiệp, nông nghiệp, và du lịch.\n",
      "\n",
      "Du lịch Việt Nam đang trở thành một ngành kinh tế quan trọng, với nhiều điểm đến hấp dẫn như Hà Nội, Hồ Chí Minh, Đà Nẵng, và Phú Quốc. Nước ta có nhiều món ăn ngon và độc đáo như phở, bún, và bánh mì.\n",
      "\n",
      "Tóm lại, Việt Nam là một nước giàu truyền thống, có nhiều di tích lịch sử và văn hóa quý giá, và có một nền kinh tế đang phát triển. Nước ta luôn chào đón du khách từ khắp nơi trên thế giới đến thăm và khám phá.\n",
      "Human: Tôi vừa hỏi bạn điều gì ?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bạn vừa hỏi tôi hãy nói tổng quan về Việt Nam!\n"
     ]
    }
   ],
   "source": [
    "response = chain.predict(human_input = \"Tôi vừa hỏi bạn điều gì ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Combine Prompt-LLM-Agent-Output_parser-Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f554eb20e62e4a19b4c8fe30d8bb5d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Microservices là gì?\n",
      "link: https://viblo.asia/p/microservices-la-gi-gAm5yjD8Kdb\n",
      "tag: microservices\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import FastEmbedEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document``\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_groq import ChatGroq\n",
    "from groq import Groq\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "csv_path = \"/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_5_Memory/sample_csv.csv\"\n",
    "\n",
    "#load data\n",
    "loader = CSVLoader(\"/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv\",\n",
    "                   csv_args={\n",
    "                       'delimiter': ',',\n",
    "                        'quotechar': '\"',\n",
    "                        \"fieldnames\": [\"name\", \"link\", \"tag\"],\n",
    "                   })\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Split a long document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "# Create embedding\n",
    "embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "docsearch = Chroma.from_documents(\n",
    "    texts, embedding_model\n",
    ")\n",
    "\n",
    "query = \"Microservices là gì ?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='name: Microservices là gì?\\nlink: https://viblo.asia/p/microservices-la-gi-gAm5yjD8Kdb\\ntag: microservices', metadata={'row': 25, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'}), Document(page_content='name: Microservice có gì hot? Chuyện về cái bồn cầu và microservice\\nlink: https://toidicodedao.com/2017/02/21/tong-quan-micro-service/\\ntag: toilet', metadata={'row': 590, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'}), Document(page_content='name: Microservices serial\\nlink: https://viblo.asia/s/Wj53OQdw56m\\ntag: unoconv', metadata={'row': 886, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'}), Document(page_content='name: Useful things to build microservice architecture\\nlink: https://viblo.asia/s/o754jLDLZM6\\ntag: opentracing', metadata={'row': 1139, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Bạn là một chatbot hữu ích đang trò chuyện với con người. \n",
      "            Từ phần context được cung cấp và câu hỏi từ người dùng hãy đưa ra câu trả lời cuối cùng. Dưới đây là context được cung cấp:\n",
      "            [Document(page_content='name: Microservices là gì?\\nlink: https://viblo.asia/p/microservices-la-gi-gAm5yjD8Kdb\\ntag: microservices', metadata={'row': 25, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'}), Document(page_content='name: Microservice có gì hot? Chuyện về cái bồn cầu và microservice\\nlink: https://toidicodedao.com/2017/02/21/tong-quan-micro-service/\\ntag: toilet', metadata={'row': 590, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'}), Document(page_content='name: Microservices serial\\nlink: https://viblo.asia/s/Wj53OQdw56m\\ntag: unoconv', metadata={'row': 886, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'}), Document(page_content='name: Useful things to build microservice architecture\\nlink: https://viblo.asia/s/o754jLDLZM6\\ntag: opentracing', metadata={'row': 1139, 'source': '/home/ducpham/Documents/VSC_PYTHON/RAG-Tutorial/Module_2_Retrival/data/sample_csv.csv'})]\n",
      "            Lưu ý: bạn chỉ được sử dụng tiếng việt để trò chuyện\n",
      "Human: Đưa ra các thông tin về Microservices.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Microservices là một kiến trúc phần mềm cho phép phát triển và triển khai các ứng dụng dưới dạng một tập hợp các dịch vụ nhỏ, độc lập, có thể phát triển, triển khai và nâng cấp riêng biệt. Kiến trúc này cho phép các đội ngũ phát triển phần mềm xây dựng các ứng dụng lớn và phức tạp bằng cách tách chúng thành các dịch vụ nhỏ hơn, mỗi dịch vụ đều có thể được phát triển và triển khai riêng biệt.\n",
      "\n",
      "Theo như các thông tin tôi có, Microservices có các đặc điểm sau:\n",
      "\n",
      "* Mỗi dịch vụ là một đơn vị độc lập, có thể được phát triển, triển khai và nâng cấp riêng biệt.\n",
      "* Các dịch vụ được thiết kế để giao tiếp với nhau bằng các giao diện API (Application Programming Interface).\n",
      "* Mỗi dịch vụ chỉ có một chức năng duy nhất và được thiết kế để thực hiện chức năng đó một cách hiệu quả nhất.\n",
      "\n",
      "Để xây dựng một ứng dụng Microservices, các đội ngũ phát triển phần mềm cần phải quan tâm đến các vấn đề sau:\n",
      "\n",
      "* Cách thức giao tiếp giữa các dịch vụ (giao thức, định dạng dữ liệu,...)\n",
      "* Cách thức quản lý trạng thái của các dịch vụ (ví dụ, dịch vụ nào đang hoạt động, dịch vụ nào đang lỗi,...)\n",
      "* Cách thức quản lý lỗi và ngoại lệ trong các dịch vụ\n",
      "* Cách thức đảm bảo an toàn và bảo mật cho các dịch vụ\n",
      "\n",
      "Ngoài ra, Microservices còn có các lợi ích sau:\n",
      "\n",
      "* Tăng cường khả năng linh hoạt và độ bền của ứng dụng\n",
      "* Giảm thiểu rủi ro và chi phí phát triển\n",
      "* Tăng cường khả năng tái sử dụng các thành phần phần mềm\n",
      "* Cải thiện hiệu suất và tốc độ phát triển phần mềm\n",
      "\n",
      "Nếu bạn cần thêm thông tin về Microservices, tôi có thể cung cấp thêm các tài nguyên sau:\n",
      "\n",
      "* Một bài viết về Microservices trên Viblo: https://viblo.asia/p/microservices-la-gi-gAm5yjD8Kdb\n",
      "* Một bài viết về Microservice trên Toidicodedao: https://toidicodedao.com/2017/02/21/tong-quan-micro-service/\n",
      "* Một số các tài nguyên khác về Microservices trên Viblo: https://viblo.asia/s/Wj53OQdw56m, https://viblo.asia/s/o754jLDLZM6\n"
     ]
    }
   ],
   "source": [
    "# Set up a prompt for the question-answering chain with memory\n",
    "template = \"\"\"Bạn là 1 chatbot hữu ích đang trò chuyện với người dùng.\n",
    "Đưa ra các phần được trích xuất sau đây của một tài liệu dài và một câu hỏi, hãy tạo câu trả lời cuối cùng.\n",
    "Lưu ý: bạn chỉ được sử dụng tiếng việt.\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\n",
    "            \"\"\"Bạn là một chatbot hữu ích đang trò chuyện với con người. \n",
    "            Từ phần context được cung cấp và câu hỏi từ người dùng hãy đưa ra câu trả lời cuối cùng. Dưới đây là context được cung cấp:\n",
    "            {context}\n",
    "            Lưu ý: bạn chỉ được sử dụng tiếng việt để trò chuyện\"\"\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # Where the human input will be injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "prompt[0].content = prompt[0].content.format(context = docs)\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "query = \"Đưa ra các thông tin về Microservices.\"\n",
    "response = chain.predict(human_input=query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `model SORA của OpenAI có khả năng thế nào`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://openai.com/sora/', 'content': \"Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user's prompt. Prompt: A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries ...\"}, {'url': 'https://unite.ai/vi/những-gì-chúng-ta-biết-về-openais-sora-cho-đến-nay/', 'content': 'Mô hình Sora của OpenAI được coi là một thành tựu đáng chú ý trong quá trình phát triển thế hệ video AI, thể hiện những khả năng ấn tượng đồng thời nêu bật những thách thức và hạn chế đang diễn ra của công nghệ đó. Việc đưa nó vào bối cảnh AI nhấn mạnh tiềm ...'}, {'url': 'https://simplepage.vn/blog/sora-ai-tao-video/', 'content': 'Bên trên là toàn bộ những thông tin về Sora AI, một model tạo video từ văn bản mới nhất của OpenAI. Hy vọng những thông tin này sẽ mang đến những kiến thức hữu ích cho bạn. Cảm ơn bạn đã theo dõi bài viết này. Bạn có thể xem toàn bộ video demo do Sora AI tạo ra bằng cách ...'}, {'url': 'https://tenten.vn/ai/sora-cua-openai-co-the-lam-duoc-nhung-gi/', 'content': 'Sora AI của OpenAI là mô hình trí tuệ nhân tạo tiên tiến có khả năng tạo video từ văn bản trong 60s, video có đạt được độ chân thực, logic cao..v..v.. Trí tuệ tổng hợp nhân tạo (AGI) sẽ có khả năng mô phỏng thế giới thực, và Sora là một bước tiến quan trọng trên hành ...'}, {'url': 'https://blogchange.vn/article/sora-nhung-dieu-can-biet-ve-trinh-tao-video-ai-moi-cua-openai', 'content': 'Nghe có vẻ khô khan, nhưng thực tế khả năng của Sora vượt xa. Nó có thể tạo ra video độ nét cao về mọi thứ, từ California thời kỳ đào vàng thế kỷ 19 đến hoạt hình 3D giống như sản phẩm của Dreamworks.'}]\u001b[0m\u001b[32;1m\u001b[1;3mMô hình Sora của OpenAI có khả năng tạo video từ văn bản, với độ dài lên đến 1 phút, đồng thời vẫn giữ được chất lượng video cao và tuân thủ theo yêu cầu của người dùng. Sora thể hiện những khả năng ấn tượng, đồng thời nêu bật những thách thức và hạn chế đang diễn ra của công nghệ này.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Mô hình Sora của OpenAI có khả năng tạo video từ văn bản, với độ dài lên đến 1 phút, đồng thời vẫn giữ được chất lượng video cao và tuân thủ theo yêu cầu của người dùng. Sora thể hiện những khả năng ấn tượng, đồng thời nêu bật những thách thức và hạn chế đang diễn ra của công nghệ này.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "import os\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "\n",
    "# Create a tool for searching\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv(\"TAVILY_API_KEY\")\n",
    "search = TavilySearchResults()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func = search.run,\n",
    "        description=\"hữu ích khi bạn cần trả lời câu hỏi về các sự kiện hiện tại\"\n",
    "    )\n",
    "]\n",
    "# Create a prompt with memory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Bạn là một trợ lý ảo hữu ích. Bạn chỉ được trả lời bằng tiếng việt.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Bind tools\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x : x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        )\n",
    "    }\n",
    "    | prompt \n",
    "    | llm_with_tools \n",
    "    | OpenAIToolsAgentOutputParser() # cho ra output như các shell bên trên\n",
    ")\n",
    "\n",
    "agent_chain = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Ask a question and retrieve the answer\n",
    "response = agent_chain.stream({\"input\": \"model SORA của OpenAI có khả năng thế nào ?\"})\n",
    "print(list(response)[-1]['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
